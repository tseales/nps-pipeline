{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# National Park Services (NPS) API Exploration","metadata":{}},{"cell_type":"markdown","source":"# README\n\n#### **Goal** \n\nThe goal of this project was to utilize the National Park Services' (NPS) API for purposes of:\n   - Practicing data pipe-lining\n   - Locally creating datasets derived from API calling\n   - Inserting Pandas DataFrames in cloud-based databases (with original intent of AWS specifically)\n   - Updating AWS database from Python via use of functions\n  \nThe code contained in this notebook mainly concerns migrating local datasets to a cloud database. Further transformation/normalization/migration occurred mainly in second example below. Due to costs of resources for AWS Redshift, with no trial period available, the first example was only utilized to the point of migrating relational tables to a AWS S3 bucket.  \n   - E.g., migrating from local table(s) -> AWS RDS -> AWS S3 -> AWS Redshift -> dbt\n   - E.g., migrating from local table(s) -> Snowflake -> dbt","metadata":{}},{"cell_type":"markdown","source":"### Commands for local install of pyscopg library & update of pip - without going to Linux terminal","metadata":{"execution":{"iopub.status.busy":"2024-05-21T15:23:14.523816Z","iopub.execute_input":"2024-05-21T15:23:14.524735Z","iopub.status.idle":"2024-05-21T15:23:14.528769Z","shell.execute_reply.started":"2024-05-21T15:23:14.524704Z","shell.execute_reply":"2024-05-21T15:23:14.527738Z"}}},{"cell_type":"code","source":"# importing packages\nimport requests\nimport pandas as pd\nimport time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n'''\n- The following will import an API Key from a function found in a private kaggle notebook... \n  - This was added as a utility script to this notebook & left for demonstration purposes\n  - Script is no longer associated with this import, for API security purposes, & the import should be uncommented/replaced if desired to be used\n'''\n# from npsapikey import get_nps_key","metadata":{"execution":{"iopub.status.busy":"2024-05-21T15:25:55.816215Z","iopub.execute_input":"2024-05-21T15:25:55.816577Z","iopub.status.idle":"2024-05-21T15:25:57.002830Z","shell.execute_reply.started":"2024-05-21T15:25:55.816550Z","shell.execute_reply":"2024-05-21T15:25:57.001892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nImports for:\n- External PostgreSQL connections (AWS)\n- Snowflake Connector \n'''\nimport psycopg as ps\n\n# Snowflake module needed for creating connection to Snowflake account & warehouse details\nimport snowflake.connector\nfrom snowflake.connector.errors import OperationalError\n# Snowflake module needed to write Pandas DF to a table\nfrom snowflake.connector.pandas_tools import write_pandas","metadata":{"execution":{"iopub.status.busy":"2024-05-19T23:44:26.832007Z","iopub.execute_input":"2024-05-19T23:44:26.832868Z","iopub.status.idle":"2024-05-19T23:44:26.838978Z","shell.execute_reply.started":"2024-05-19T23:44:26.832831Z","shell.execute_reply":"2024-05-19T23:44:26.837211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nThe following code should be uncommented & used if utilizing a local Jupyter Notebook...\n  - Kaggle utilizes 'Utility Scripts' for ability to import functions from another notebook\n  - Local use of Jupyter Notebook utilizes the 'run' magic command to import functions from another notebook\n'''\n\n# # importing Jupyter notebook storing API Key\n# %run NPSAPIKey.ipynb\n\n# # importing API Key\n# API_KEY = get_nps_key()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T23:55:49.517901Z","iopub.execute_input":"2024-05-19T23:55:49.518340Z","iopub.status.idle":"2024-05-19T23:55:49.625812Z","shell.execute_reply.started":"2024-05-19T23:55:49.518308Z","shell.execute_reply":"2024-05-19T23:55:49.624097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# storing API Key from import of Kaggle utility script \nAPI_KEY = get_nps_key()","metadata":{"execution":{"iopub.status.busy":"2024-05-21T00:44:56.122824Z","iopub.execute_input":"2024-05-21T00:44:56.123380Z","iopub.status.idle":"2024-05-21T00:44:56.128189Z","shell.execute_reply.started":"2024-05-21T00:44:56.123346Z","shell.execute_reply":"2024-05-21T00:44:56.127071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nFunction for creating 'parks' Pandas DataFrame and storing relative Park details, including: Name, ID, Code, Website URL,\nLatitude & Longitude coordinates, physical location's state abbreviation (e.g., NY, NJ)\n'''\ndef get_park_details(parks_df):\n    # creating empty list for storing park data\n    temp = []\n    # making API call\n    response = requests.get('https://developer.nps.gov/api/v1/parks?&limit=480&start=0&api_key='+API_KEY).json()\n    \n    # setting pause-timer to allow enough time for data to flow from API\n    time.sleep(2)\n    \n    for park in response['data']:    \n        park_id = park['id']\n        park_name = park['fullName']\n        park_code = park['parkCode']\n        park_url = park['url']\n        park_lat = park['latitude']\n        park_long = park['longitude']\n        park_state = park['addresses'][0]['stateCode']\n\n        # appending park data to empty list\n        temp.append({'park_id':park_id, 'park_name': park_name, 'park_code':park_code,\n                                    'park_state':park_state, 'park_url':park_url, 'park_lat':park_lat,\n                                    'park_long':park_long})\n\n    # creating temporary dataframe from temp list\n    temp_df = pd.DataFrame(temp)\n    # appending details from temp DF to parks DF \n    parks_df = pd.concat([parks_df, temp_df]).reset_index(drop=True)\n\n    return parks_df","metadata":{"execution":{"iopub.status.busy":"2024-05-21T00:44:59.611307Z","iopub.execute_input":"2024-05-21T00:44:59.611683Z","iopub.status.idle":"2024-05-21T00:44:59.620039Z","shell.execute_reply.started":"2024-05-21T00:44:59.611653Z","shell.execute_reply":"2024-05-21T00:44:59.619022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nFunction for creating and populating DataFrames for both:\n  (1) Unique 'activities' found in 'parks' data received from API call - storing relative Activity's Name and ID\n  (2) Parks-Activity relationship - storing associated parkID with activityID for activities available in the associated park\n'''\ndef get_activities_details(activities_df, park_activities_df):\n    # empty list for storing activities\n    activities_list = []\n    # empty list for storing park-activity relationships\n    park_act_list = []\n    \n    # making API call\n    resp = requests.get('https://developer.nps.gov/api/v1/parks?&limit=480&start=0&api_key='+API_KEY).json()\n    \n    # setting pause-timer to allow enough time for data to flow from API\n    time.sleep(2)\n    \n    # loops through parks data and stores list of activities found in each park\n    for park in resp['data']:\n        # empty list for storing activity IDs\n        actid_list = []\n        park_id = park['id']\n        for activity in park['activities']:\n            act_name = activity['name']\n            act_id = activity['id']\n            # adds activity & asssociated park ID to list if activity ID isn't found in list of activity IDs\n            if act_id not in actid_list:\n                activities_list.append({'act_id':act_id, 'act_name':act_name})\n                park_act_list.append({'park_id': park_id, 'act_id':act_id})\n                actid_list.append(act_id)\n                \n    # creating temporary activity dataframe from temp list\n    temp_activity_df = pd.DataFrame(activities_list)\n    # creating temporary park-activity dataframe from park-activity relationship list\n    temp_park_activity_df = pd.DataFrame(park_act_list)\n    \n    # appending details from temp activity DF to activities DF \n    activities_df = pd.concat([activities_df, temp_activity_df]).reset_index(drop=True)\n    # appending details from temp park-activity relationship DF to park-activity DF \n    park_activities_df = pd.concat([park_activities_df, temp_park_activity_df]).reset_index(drop=True)\n    \n    return activities_df, park_activities_df","metadata":{"execution":{"iopub.status.busy":"2024-05-21T00:44:59.968031Z","iopub.execute_input":"2024-05-21T00:44:59.968920Z","iopub.status.idle":"2024-05-21T00:44:59.980596Z","shell.execute_reply.started":"2024-05-21T00:44:59.968877Z","shell.execute_reply":"2024-05-21T00:44:59.979325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nFunction for creating and populating DataFrames for both:\n  (1) Unique 'topics' found in 'parks' data received from API call - storing relative Topic's Name and ID\n  (2) Parks-Topics relationship, storing associated parkID with topicID for topics available in the associated park\n         (e.g., American Revolution, Music, Women's History, etc.) \n'''\ndef get_topics_details(topics_df, park_topics_df):\n    # empty list for storing topics\n    topics_list = []\n    # empty list for storing park-topics relationships\n    park_top_list = []\n    \n    # making API call\n    resp = requests.get('https://developer.nps.gov/api/v1/parks?&limit=480&start=0&api_key='+API_KEY).json()\n    \n    # setting pause-timer to allow enough time for data to flow from API\n    time.sleep(2)\n    \n    # loops through parks data and stores list of topics found in each park\n    for park in resp['data']:\n        # empty list for storing topics' IDs\n        topid_list = []\n        park_id = park['id']\n        for topic in park['topics']:\n            top_name = topic['name']\n            top_id = topic['id']\n            # adds topic & asssociated park ID to list if topic ID isn't found in list of topic IDs\n            if top_id not in topid_list:\n                topics_list.append({'top_id':top_id, 'top_name':top_name})\n                park_top_list.append({'park_id': park_id, 'top_id':top_id})\n                topid_list.append(top_id)\n    \n    # creating temporary topics dataframe from temp list\n    temp_topics_df = pd.DataFrame(topics_list)\n    # creating temporary park-topics dataframe from park-topics relationship list\n    temp_park_topics_df = pd.DataFrame(park_top_list)\n    \n    # appending details from temp topics DF to topics DF \n    topics_df = pd.concat([topics_df, temp_topics_df]).reset_index(drop=True)\n    # appending details from temp park-topics relationship DF to park-topics DF \n    park_topics_df = pd.concat([park_topics_df, temp_park_topics_df]).reset_index(drop=True)\n\n    return topics_df, park_topics_df","metadata":{"execution":{"iopub.status.busy":"2024-05-21T00:45:03.375007Z","iopub.execute_input":"2024-05-21T00:45:03.375393Z","iopub.status.idle":"2024-05-21T00:45:03.384420Z","shell.execute_reply.started":"2024-05-21T00:45:03.375363Z","shell.execute_reply":"2024-05-21T00:45:03.383052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Main","metadata":{}},{"cell_type":"code","source":"# building dataframes\nparks_df = pd.DataFrame(columns=['park_id', 'park_name', 'park_code', 'park_state', 'park_url', 'park_lat', 'park_long'])\nactivities_df = pd.DataFrame(columns=['act_id', 'act_name'])\npark_activities_df = pd.DataFrame(columns=['park_id', 'act_id'])\ntopics_df = pd.DataFrame(columns=['top_id', 'top_name'])\npark_topics_df = pd.DataFrame(columns=['park_id', 'top_id'])\n\n# calling helper functions to populate created dataframes\nparks_df = get_park_details(parks_df)\nactivities_df, park_activities_df = get_activities_details(activities_df, park_activities_df)\ntopics_df, park_topics_df = get_topics_details(topics_df, park_topics_df)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T00:45:03.757364Z","iopub.execute_input":"2024-05-21T00:45:03.758009Z","iopub.status.idle":"2024-05-21T00:45:15.486547Z","shell.execute_reply.started":"2024-05-21T00:45:03.757979Z","shell.execute_reply":"2024-05-21T00:45:15.485615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dropping duplicates from activities DataFrame to ensure only unique values\nactivities_df = activities_df.drop_duplicates(subset=['act_id']).reset_index(drop=True)\n\n# dropping duplicates from topics DataFrame to ensure only unique values\ntopics_df = topics_df.drop_duplicates(subset=['top_id']).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T00:45:18.827065Z","iopub.execute_input":"2024-05-21T00:45:18.828064Z","iopub.status.idle":"2024-05-21T00:45:18.846119Z","shell.execute_reply.started":"2024-05-21T00:45:18.828028Z","shell.execute_reply":"2024-05-21T00:45:18.844984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# AWS PostgreSQL Database","metadata":{"execution":{"iopub.status.busy":"2024-05-21T16:44:51.213640Z","iopub.execute_input":"2024-05-21T16:44:51.214369Z","iopub.status.idle":"2024-05-21T16:44:51.219221Z","shell.execute_reply.started":"2024-05-21T16:44:51.214301Z","shell.execute_reply":"2024-05-21T16:44:51.218105Z"}}},{"cell_type":"markdown","source":"## Pushing Pandas Dataframes to AWS PostgreSQL Database","metadata":{}},{"cell_type":"code","source":"'''\nThe following code should be uncommented & used if utilizing a local Jupyter Notebook\n  - This is intended to pull stored details for created AWS PostgreSQL database, for remote local connection \n'''\n\n# # importing Jupyter notebook storing NPS DB details\n# %run NPSDB.ipynb\n\n# # importing needed DB details\n# host, dbname, user, pw, port = get_nps_db()\n\n# #initializing connection variable\n# conn=None","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# connects to remote AWS DB, otherwise raises error\ntry:\n    conn = ps.connect(host=host, dbname=dbname, user=user, password=pw, port=port, autocommit=True)\nexcept ps.OperationalError as e:\n    raise e\nelse:\n    print('Connected!')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# opens cursor to perform database operations\ncur = conn.cursor()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating SQL Tables","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"cell_type":"code","source":"# create parks table\ncreate_parks_table = (\"\"\"CREATE TABLE IF NOT EXISTS parks (\n                    park_id VARCHAR(255) PRIMARY KEY,\n                    park_name TEXT NOT NULL,\n                    park_state TEXT,\n                    park_code VARCHAR(255),\n                    park_url TEXT,\n                    park_lat VARCHAR(255),\n                    park_long VARCHAR(255)\n                )\"\"\")\n\n# execute sql command\ncur.execute(create_parks_table)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create activities table if it doesn't exist already\ncreate_activities_table = (\"\"\"CREATE TABLE IF NOT EXISTS activities (\n                    activity_id VARCHAR(255) PRIMARY KEY,\n                    activity_name TEXT NOT NULL\n                )\"\"\")\n\n# execute sql command\ncur.execute(create_activities_table)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create parks_activities relationships table\ncreate_park_activities_table = (\"\"\"CREATE TABLE IF NOT EXISTS park_activities (\n                    park_id VARCHAR(255),\n                    activity_id VARCHAR(255),\n                    PRIMARY KEY(park_id, activity_id)\n                )\"\"\")\n\n#execute sql command\ncur.execute(create_park_activities_table)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create topics table if it doesn't exist already\ncreate_topics_table = (\"\"\"CREATE TABLE IF NOT EXISTS topics (\n                    topic_id VARCHAR(255) PRIMARY KEY,\n                    topic_name TEXT NOT NULL\n                )\"\"\")\n\n# execute sql command\ncur.execute(create_topics_table)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create parks_topics relationships table\ncreate_park_topics_table = (\"\"\"CREATE TABLE IF NOT EXISTS park_topics (\n                    park_id VARCHAR(255),\n                    topic_id VARCHAR(255),\n                    PRIMARY KEY(park_id, topic_id)\n                )\"\"\")\n\n#execute sql command\ncur.execute(create_park_topics_table)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Insert & Update Parks Data","metadata":{}},{"cell_type":"code","source":"# function for inserting and updating data into parks_df - to be used with 'parks' SQL table\ndef update_parks_db(cur, parks_df):\n    # empty list for storing new IDs\n    new_row = []\n    \n    for i, row in parks_df.iterrows():\n        if check_park_existence(cur, row['park_id']): # if park already exists, update SQL table\n            update_park_row(cur, row['park_name'], row['park_state'], row['park_code'], row['park_url'], row['park_lat'], row['park_long'], row['park_id'])\n        else: # park doesn't exist - append to SQL table\n            # append details of new row to list for creating dataframe\n            new_row.append(row)\n            \n    tmp_parks_df = pd.DataFrame(new_row)\n    return tmp_parks_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checks if park exists in 'parks' SQL table\ndef check_park_existence(cur, park_id):\n    query = (\"\"\"SELECT * FROM parks WHERE park_id = (%s);\"\"\")\n    cur.execute(query, (park_id,))\n\n    return cur.fetchone() is not None","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# updates park's row in 'parks' SQL table, if already exists\ndef update_park_row(cur, park_name, park_state, park_code, park_url, park_lat, park_long, park_id):\n    query = (\"\"\"UPDATE parks SET\n                park_name = (%s),\n                park_state = (%s),\n                park_code = (%s),\n                park_url = (%s),\n                park_lat = (%s),\n                park_long = (%s)\n                WHERE park_id = (%s);\"\"\")\n    params = (park_name, park_state, park_code, park_url, park_lat, park_long, park_id,)\n\n    cur.execute(query, params)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nFunction for inserting, row-by-row, the new details contained within new_parks_df\n    - This will call the function 'insert_into_parks_table' for actually querying the 'parks' SQL table\n'''\ndef append_parksDF_to_table(cur, new_parks_df):\n    # inserts parks row-by-row\n    for i, row in new_parks_df.iterrows():\n        insert_into_parks_table(cur, row['park_id'], row['park_name'], row['park_state'], row['park_code'], row['park_url'], row['park_lat'], row['park_long'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# queries the 'parks' SQL table to insert new row(s) of parks gathered\ndef insert_into_parks_table(cur, park_id, park_name, park_state, park_code, park_url, park_lat, park_long):\n    query = (\"\"\"INSERT INTO parks (park_id, park_name, park_state, park_code, park_url, park_lat, park_long)\n                VALUES (%s, %s, %s, %s, %s, %s, %s);\"\"\")\n    params = (park_id, park_name, park_state, park_code, park_url, park_lat, park_long,)\n\n    cur.execute(query, params)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# calls function to store data for any new parks\nnew_parks_df = update_parks_db(cur, parks_df)\n\n# calls function to insert any new rows into 'parks' SQL table\nappend_parksDF_to_table(cur, new_parks_df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Insert & Update Activites Data","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"cell_type":"code","source":"# function for inserting and updating data into activities_df - to be used with 'activities' SQL table\ndef update_activities_db(cur, activities_df):\n    # empty list for storing new IDs\n    new_row = []\n    \n    for i, row in activities_df.iterrows():\n        if check_activity_existence(cur, row['act_id']): # if activity already exists, update SQL table\n            update_activity_row(cur, row['act_name'])\n        else: # activity doesn't exist - append to SQL table\n            # append details of new row to list for creating dataframe\n            new_row.append(row)\n            \n    tmp_activity_df = pd.DataFrame(new_row)\n    return tmp_activity_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checks if activity exists in 'activities' SQL table\ndef check_activity_existence(cur, act_id):\n    query = (\"\"\"SELECT * FROM activities WHERE activity_id = (%s);\"\"\")\n    cur.execute(query, (act_id,))\n\n    return cur.fetchone() is not None","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# updates activity's row in 'activities' SQL table, if already exists\ndef update_activity_row(cur, act_name, act_id):\n    query = (\"\"\"UPDATE activities SET\n                activity_name = (%s),\n                WHERE activity_id = (%s);\"\"\")\n    params = (act_name, act_id)\n\n    cur.execute(query, params)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nFunction for inserting, row-by-row the new details contained within new_activities_df\n    - This will call the function 'insert_into_activities_table' for actually querying the 'activities' SQL table\n'''\ndef append_activitiesDF_to_table(cur, new_activities_df):\n    # inserts activities row-by-row\n    for i, row in new_activities_df.iterrows():\n        insert_into_activities_table(cur, row['act_id'], row['act_name'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# queries the 'activities' SQL table to insert new row(s) of activities gathered\ndef insert_into_activities_table(cur, act_id, act_name):\n    query = (\"\"\"INSERT INTO activities (activity_id, activity_name)\n                VALUES (%s, %s);\"\"\")\n    params = (act_id, act_name,)\n\n    cur.execute(query, params)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# calls function to store data for any new activites\nnew_activities_df = update_activities_db(cur, activities_df)\n\n# calls function to insert any new rows into 'activities' SQL table\nappend_activitiesDF_to_table(cur, new_activities_df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Insert & Update Topics Data","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"cell_type":"code","source":"# function for inserting and updating data into topics_df - to be used with 'topics' SQL table\ndef update_topics_db(cur, topics_df):\n    # empty list for storing new IDs\n    new_row = []\n    \n    for i, row in topics_df.iterrows():\n        if check_topic_existence(cur, row['top_id']): # if topic already exists, update SQL table\n            update_topic_row(cur, row['top_name'])\n        else: # topic doesn't exist - append to SQL table\n            # append details of new row to list for creating dataframe\n            new_row.append(row)\n            \n    tmp_topic_df = pd.DataFrame(new_row)\n    return tmp_topic_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checks if topic exists in 'topics' SQL table\ndef check_topic_existence(cur, top_id):\n    query = (\"\"\"SELECT * FROM topics WHERE topic_id = (%s);\"\"\")\n    cur.execute(query, (top_id,))\n\n    return cur.fetchone() is not None","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# updates topic's row in 'topics' SQL table, if already exists\ndef update_topic_row(cur, top_name, top_id):\n    query = (\"\"\"UPDATE topics SET\n                topic_name = (%s),\n                WHERE topic_id = (%s);\"\"\")\n    params = (top_name, top_id)\n\n    cur.execute(query, params)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nFunction for inserting, row-by-row the new details contained within new_topics_df\n    - This will call the function 'insert_into_topics_table' for actually querying the 'topics' SQL table\n'''\ndef append_topicDF_to_table(cur, new_topics_df):\n    # inserts topics row-by-row\n    for i, row in new_topics_df.iterrows():\n        insert_into_topics_table(cur, row['top_id'], row['top_name'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# queries the 'topics' SQL table to insert new row(s) of topics gathered\ndef insert_into_topics_table(cur, top_id, top_name):\n    query = (\"\"\"INSERT INTO topics (topic_id, topic_name)\n                VALUES (%s, %s);\"\"\")\n    params = (top_id, top_name,)\n\n    cur.execute(query, params)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# calls function to store data for any new topics\nnew_topics_df = update_topics_db(cur, topics_df)\n\n# calls function to insert any new rows into 'topics' SQL table\nappend_topicDF_to_table(cur, new_topics_df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Insert & Update Park_Activties Data","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"cell_type":"code","source":"# function for inserting and updating data into park_activities_df - to be used with 'parks_activities' SQL table\ndef update_park_activities_db(cur, park_activities_df):\n    # empty list for storing new IDs\n    new_row = []\n    \n    for i, row in park_activities_df.iterrows():\n        if check_park_activity_existence(cur, row['park_id'], row['act_id']) is None: # if park_activity relation doesn't exist - append to SQL table\n            # append details of new row to list for creating dataframe\n            new_row.append(row)\n        else:\n            print('Nothing to insert')\n            \n    tmp_park_activities_df = pd.DataFrame(new_row)\n    return tmp_park_activities_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checks if relationship exists in 'park_activities' SQL table\ndef check_park_activity_existence(cur, park_id, act_id):\n    query = (\"\"\"SELECT * FROM park_activities WHERE park_id = (%s) AND activity_id = (%s);\"\"\")\n    cur.execute(query, (park_id, act_id,))\n\n    return cur.fetchone()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nFunction for inserting, row-by-row the new details contained within new_parkactivities_df\n    - This will call the function 'insert_into_parkactivities_table' for actually querying the 'park_activities' SQL table\n'''\ndef append_parkactivitiesDF_to_table(cur, new_parkactivities_df):\n    # inserts park_activity relation row-by-row\n    for i, row in new_parkactivities_df.iterrows():\n        insert_into_parkactivities_table(cur, row['park_id'], row['act_id'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# queries the 'park_activities' SQL table to insert new row(s) of relations gathered\ndef insert_into_parkactivities_table(cur, park_id, act_id):\n    query = (\"\"\"INSERT INTO park_activities (park_id, activity_id)\n                VALUES (%s, %s);\"\"\")\n    params = (park_id, act_id,)\n\n    cur.execute(query, params)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# calls function to store data for any new park_activites relations\nnew_parkactivities_df = update_park_activities_db(cur, park_activities_df)\n\n# calls function to insert any new rows into 'park_activities' SQL table\nappend_parkactivitiesDF_to_table(cur, new_parkactivities_df)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Insert & Update Park_Topics Data","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"cell_type":"code","source":"# function for inserting and updating data into park_topics_df - to be used with 'park_topics' SQL table\ndef update_park_topics_db(cur, park_topics_df):\n    # empty list for storing new IDs\n    new_row = []\n    \n    for i, row in park_topics_df.iterrows():\n        if check_park_topic_existence(cur, row['park_id'], row['top_id']) is None: # if park_topics relation doesn't exist - append to SQL table\n            # append details of new row to list for creating dataframe\n            new_row.append(row)\n        else:\n            print('Nothing to insert')\n            \n    tmp_park_topics_df = pd.DataFrame(new_row)\n    return tmp_park_topics_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checks if relationship exists in 'park_topics' SQL table\ndef check_park_topic_existence(cur, park_id, top_id):\n    query = (\"\"\"SELECT * FROM park_topics WHERE park_id = (%s) AND topic_id = (%s);\"\"\")\n    cur.execute(query, (park_id, top_id,))\n\n    return cur.fetchone()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nFunction for inserting, row-by-row the new details contained within new_parktopics_df\n    - This will call the function 'insert_into_parktopics_table' for actually querying the 'park_topics' SQL table\n'''\ndef append_parktopicsDF_to_table(cur, new_parktopics_df):\n    # inserts park_topic relation row-by-row\n    for i, row in new_parktopics_df.iterrows():\n        insert_into_parktopics_table(cur, row['park_id'], row['top_id'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# queries the 'park_topics' SQL table to insert new row(s) of relations gathered\ndef insert_into_parktopics_table(cur, park_id, top_id):\n    query = (\"\"\"INSERT INTO park_topics (park_id, topic_id)\n                VALUES (%s, %s);\"\"\")\n    params = (park_id, top_id,)\n\n    cur.execute(query, params)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# calls function to store data for any new park_topics relations\nnew_parktopics_df = update_park_topics_db(cur, park_topics_df)\n\n# calls function to insert any new rows into 'park_topics' SQL table\nappend_parktopicsDF_to_table(cur, new_parktopics_df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# manually close connection after performing needed AWS operations \nconn.close()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Snowflake","metadata":{}},{"cell_type":"markdown","source":"## Pushing Pandas Dataframes to Snowflake Database\n- Only INSERT operations performed, there was no intent for records to be updated, so code does not reflect this","metadata":{}},{"cell_type":"code","source":"# #local install for Pandas use in Snowflake\n\n# !pip install \"snowflake-connector-python[pandas]\"","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nThe following code should be uncommented & used if utilizing a local Jupyter Notebook\n  - This is intended to pull stored details for created Snowflake warehouse, for remote local connection \n'''\n# # getting Snowflake Auth & Warehouse details\n# %run NPS_SNOWFLAKE.ipynb\n\n# user, password, account, warehouse, database, schema = get_nps_snowflake()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# opening a connection to created NPS database in Snowflake, otherwise raises error\ntry:\n    conn = snowflake.connector.connect(\n    user = user,\n    password = password,\n    account = account,\n    warehouse = warehouse,\n    database = database,\n    schema = schema\n    )\nexcept OperationalError as e:\n    raise e\nelse:\n    print('Connected!')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# opening a cursor for performing database operations\ncur = conn.cursor()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nManual setting of Warehouse, Database & Schema to be used\n    - Snowflake seems to need this regardless of if initial cursor connection is setup with these params.\n'''\ncur.execute(\"USE WAREHOUSE NPS_WH\")\ncur.execute(\"USE DATABASE NPS_NATLPARKS\")\ncur.execute(\"USE SCHEMA NPS_NATLPARKS.PUBLIC\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating SQL Tables","metadata":{}},{"cell_type":"code","source":"# create parks table\ncreate_parks_table = (\"\"\"CREATE OR REPLACE TABLE parks(\n                        park_id VARCHAR(255) PRIMARY KEY,\n                        park_name TEXT NOT NULL,\n                        park_state TEXT,\n                        park_code VARCHAR(255),\n                        park_url TEXT,\n                        park_lat VARCHAR(255),\n                        park_long VARCHAR(255)\n                        )\"\"\")\n\n# execute sql command\ncur.execute(create_parks_table)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create activities table \ncreate_activities_table = (\"\"\"CREATE OR REPLACE TABLE activities (\n                    activity_id VARCHAR(255) PRIMARY KEY,\n                    activity_name VARCHAR(255) NOT NULL\n                )\"\"\")\n\n# execute sql command\ncur.execute(create_activities_table)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create park_activities relationships table\ncreate_park_activities_table = (\"\"\"CREATE OR REPLACE TABLE park_activities (\n                    park_id VARCHAR(255),\n                    activity_id VARCHAR(255),\n                    PRIMARY KEY(park_id, activity_id)\n                )\"\"\")\n\n#execute sql command\ncur.execute(create_park_activities_table)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create topics table if it doesn't exist already\ncreate_topics_table = (\"\"\"CREATE OR REPLACE TABLE topics (\n                    topic_id VARCHAR(255) PRIMARY KEY,\n                    topic_name VARCHAR(255) NOT NULL\n                )\"\"\")\n\n# execute sql command\ncur.execute(create_topics_table)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create park_topics relationships table\ncreate_park_topics_table = (\"\"\"CREATE OR REPLACE TABLE park_topics (\n                    park_id VARCHAR(255),\n                    topic_id VARCHAR(255),\n                    PRIMARY KEY(park_id, topic_id)\n                )\"\"\")\n\n#execute sql command\ncur.execute(create_park_topics_table)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inserting Data into created Snowflake Tables","metadata":{}},{"cell_type":"code","source":"\"\"\"\nBecause Snowflake is case-sensitive and converts high-level nomenclatures (i.e., warehouse, database-name, schema, column-headers)\nto upper-case, it is necessary to convert pandas DFs column headers to upper-case for avoiding a ProgramError (invalid identifier)\n\"\"\"\nparks_df.columns = parks_df.columns.str.upper()\nactivities_df.columns = activities_df.columns.str.upper()\npark_activities_df.columns = park_activities_df.columns.str.upper()\ntopics_df.columns = topics_df.columns.str.upper()\npark_topics_df.columns = park_topics_df.columns.str.upper()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# insert to PARKS table & print success-status of write\npark_success, park_nchunks, park_nrows, _ = write_pandas(conn, parks_df, 'PARKS')\n\npark_success","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# renaming columns to match Snowflake target (without impacting AWS code in first sections)\nactivities_df = activities_df.rename(columns={'ACT_ID': 'ACTIVITY_ID', 'ACT_NAME':'ACTIVITY_NAME'})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\n# insert to ACTIVITIES table & print success-status of write\nactivities_success, activities_nchunks, activities_nrows, _ = write_pandas(conn, activities_df, 'ACTIVITIES')\n\nactivities_success","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# renaming column to match Snowflake target (without impacting AWS code in first sections)\npark_activities_df = park_activities_df.rename(columns={'ACT_ID':'ACTIVITY_ID'})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\n# insert to PARK_ACTIVITIES table & print success-status of write\npark_activities_success, park_activities_nchunks, park_activities_nrows, _ = write_pandas(conn, park_activities_df, 'PARK_ACTIVITIES')\n\npark_activities_success","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# renaming columns to match Snowflake target (without impacting AWS code in first sections)\ntopics_df = topics_df.rename(columns={'TOP_ID':'TOPIC_ID', 'TOP_NAME':'TOPIC_NAME'})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\n# insert to TOPICS table & print success-status of write\ntopics_success, topics_nchunks, topics_nrows, _ = write_pandas(conn, topics_df, 'TOPICS')\n\ntopics_success","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# renaming column to match Snowflake target (without impacting AWS code in first sections)\npark_topics_df = park_topics_df.rename(columns={'TOP_ID':'TOPIC_ID'})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\n# insert to PARK_TOPICS table & print success-status of write\npark_topics_success, park_topics_nchunks, park_topics_nrows, _ = write_pandas(conn, park_topics_df, 'PARK_TOPICS')\n\npark_topics_success","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# manually close connection after performing needed Snowflake operations \nconn.close()","metadata":{},"execution_count":null,"outputs":[]}]}